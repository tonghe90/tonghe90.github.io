
<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Tong He</title>
	<meta content="Tong He, tonghe90.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight:bold;
}

ul { 
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight: bold;
  color: #FF0000;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-left: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');



</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script>
<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="Tonghe" style="float: left; padding-left: .01em; height: 140px;" src="tonghe_5.jpg" />
<div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Tong He</span><br />
<span><strong>Email  </strong>: tonghe90[at]gmail[dot]com</span> <br /> 
<span>I am now a Research Fellow at Shanghai AI Lab, working with <a href='https://wlouyang.github.io/'>Prof. Ouyang Wanli</a>  and <a href='http://mmlab.siat.ac.cn/yuqiao/'>Prof. Qiao Yu </a>. I was a Research Fellow at <a href='https://www.adelaide.edu.au/aiml/'> Australian Institute for Machine Learning (AIML)</a>, the University of Adelaide, working with <a href='http://cs.adelaide.edu.au/~chhshen/'>Prof. Chunhua Shen</a> and <a href='https://cs.adelaide.edu.au/~hengel/'>Prof. Anton van den Hengel</a> </span>
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
<div class="section">
<h2>(<a href='https://scholar.google.com/citations?user=kWADCMUAAAAJ&hl=zh-CN'>Google scholar</a>)</h2>
<div class="paper">

I got my PhD in computer science at the <a href='https://www.adelaide.edu.au/'>University of Adelaide</a> and supervised by <a href='http://cs.adelaide.edu.au/~chhshen/'>Chunhua Shen</a>. I was a visiting student at <a href='http://mmlab.ie.cuhk.edu.hk/people.html#shenzhen'>MMLAB of the Chinese University of Hong Kong at Shenzhen</a> under the supervision of Dr.<a href='http://www.whuang.org/'>Weilin Huang</a> and Prof.<a href='http://mmlab.siat.ac.cn/yuqiao/'>Yu Qiao</a>. 

<alert>We are looking for self-motivated PhD students (joint PhD program with SJTU, FDU, ZJU, USTC etc) and interns. If you are interested in joining us, please feel free to contact me with your CV!</alert>

<br> <br>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
    <li> June, 2024: Five papers have been accepted by ECCV2024 </li>
    <li> Ranked as Worldwide Top 2% Scientists by Stanford University (2023.10)</li>
    <li> Mar, 2024: Four papers have been accepted by CVPR2024 </li>
    <li> July, 2023: One paper on long-tail object recognition has been accepted by T-PAMI </li>
    <li> July, 2023: One paper on point cloud pretraining (Ponder) has been accepted by ICCV2023 </li>
    <li> Mar, 2023: One paper on point cloud pretraining (CP3) has been accepted by T-PAMI </li>
    <li> Mar, 2023: Four papers have been accepted by CVPR23 </li>
    <li> Oct, 2022: One paper has been accepted by SIGGRAPH ASIA. </li>
    <li> Oct, 2022: The extended version of DyCo3D has been accepted by T-PAMI </li>
    <li> July, 2022: One paper has been accepted by ECCV22 </li>
    <li> April, 2022: Check our latest instance segmentation <a href='https://arxiv.org/abs/2204.11402/'>paper</a> for 3D point cloud. </li>
    <li> March, 2021: One T-PAMI has been accepted. </li>
    <li> March, 2021: One IJCV has been accepted. </li>
    <li> March, 2021: Two <a href='cvpr2021.thecvf.com//'>CVPR</a> papers have been accepted. </li>
    <li> Nov, 2020: Got Ph.D degree and my thesis was awarded <a href='https://www.adelaide.edu.au/graduatecentre/current-students/your-thesis-examination/research-student-excellence-awards'>the Dean’s Commendation for Doctoral Thesis Excellence</a>. </li>
    <li> Oct, 2020: The extended version of FCOS is accepted by T-PAMI. </li>
    <li> July, 2020: Two <a href='https://eccv2020.eu///'>ECCV</a> papers have been accepted. </li>
    <li> March, 2020: One <a href='cvpr2020.thecvf.com//'>CVPR</a> paper has been accepted. </li>
<!--
    <li> July, 2019: One <a href='iccv2019.thecvf.com//'>ICCV</a> paper has been accepted. </li>
    <li> May, 2019: We attend <a href='https://rrc.cvc.uab.es/?ch=12&com=evaluation&task=3'>ICDAR 2019 Robust Reading Challenge on Reading Chinese Text on Signboard</a>and won the 1st place in text line detection task. The related paper can be found <a href='https://arxiv.org/pdf/1912.09629.pdf'>here</a>. </li>
    <li> Mar, 2019: We attend <a href='https://blogs.adelaide.edu.au/machine-learning/'>AI edge contest</a> and won the 3rd place out of 90 teams.   </li>
    <li> Mar, 2019: Two <a href='cvpr2019.thecvf.com/'>CVPR</a> papers have been accepted. </li>
    <li> Mar, 2018: One <a href='cvpr2018.thecvf.com/'>CVPR</a> paper has been accepted. </li>
    <li> July, 2017: One <a href='www.pamitc.org/iccv17/'>ICCV</a> paper has been accepted. </li>
    <li> April, 2017: Come to Adelaide University for PhD career </li>
	<li> July 13, 2016: One <a href='www.pamitc.org/eccv16/'>ECCV</a> paper has been accepted. </li>
    <li> March 8, 2016: One <a href='http://arxiv.org/abs/1510.03283'>TIP</a> paper has been accepted. </li>
    <li> June 5, 2015: We are the 2nd winner for Large-scale Scene Understanding Challenge, on <a href='http://lsun.cs.princeton.edu/leaderboard/'>Scene Classification</a> at CVPR 2015.
-->
    </li>
    
    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Publications</h2>

<div class="paper" id="gvgen"><img class="paper" src="papers/gvgen.png" title="GVGEN: Text-to-3D Generation with Volumetric Representation" />
<div> <strong>GVGEN: Text-to-3D Generation with Volumetric Representation
</strong><br />
X. He, J. Chen, S. Peng, D. Huang, Y. Li, X. Huang, C. Yuan, W. Ouyang and <strong>T. He*</strong>. <br />
ECCV 2024,
<a href='https://arxiv.org/pdf/2403.12957.pdf'>[PDF]</a>
<a href='https://GVGEN.github.io/'>[code]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="3dagent"><img class="paper" src="papers/3dagent.png" title="Agent3D-Zero: An Agent for Zero-shot 3D Understanding" />
<div> <strong>Agent3D-Zero: An Agent for Zero-shot 3D Understanding
</strong><br />
S. Zhang, D. Huang, J. Deng, S. Tang, W. Ouyang, <strong>T. He</strong> and Y. Zhang. <br />
ECCV 2024,
<a href='https://arxiv.org/pdf/2403.11835.pdf'>[PDF]</a>
<a href='https://zhangsha1024.github.io/Agent3D-Zero/'>[code]</a>
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="dettool"><img class="paper" src="papers/dettool.png" title="DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM" />
<div> <strong>DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM
</strong><br />
Y. Wu, Y. Wang, S. Tang, W. Wu, <strong>T. He</strong>, W. Ouyang, J. Wu and P. Torr. <br />
ECCV 2024,
<a href='https://arxiv.org/pdf/2403.12488.pdf'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="pixelgs"><img class="paper" src="papers/pixelgs.png" title="Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting" />
<div> <strong>Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting
</strong><br />
Z. Zhang, W. Hu, Y. Liao, <strong>T. He</strong> and H. Zhao. <br />
ECCV 2024,
<a href='https://arxiv.org/pdf/2403.15530.pdf'>[PDF]</a>
<a href='https://pixelgs.github.io/'>[code]</a>
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="predbench"><img class="paper" src="papers/partialft.png" title="PredBench: Benchmarking Spatio-Temporal Prediction across Diverse Disciplines" />
<div> <strong>PredBench: Benchmarking Spatio-Temporal Prediction across Diverse Disciplines
</strong><br />
Z. Wang, Z. Lu, D. Huang, <strong>T. He</strong>, X. Liu, W. Ouyang and L. Bai <br />
ECCV 2024,
<a href='https://arxiv.org/abs/2407.08418'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>




<div class="paper" id="pointcloudmatters"><img class="paper" src="papers/pointcloudmatters.png" title="Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning" />
<div> <strong>Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning
</strong><br />
H. Zhu, Y. Wang, D. Huang, W. Ye, W. Ouyang and <strong>T. He*</strong>. <br />
arxiv 2024,
<a href='https://arxiv.org/pdf/2402.02500.pdf'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="unipad"><img class="paper" src="papers/unipad.png" title="UNIPAD: A UNIVERSAL PRE-TRAINING PARADIGM FOR AUTONOMOUS DRIVING" />
<div> <strong>UniPad: A Universal Pre-Training Paradigm For Autonomous Driving
</strong><br />
H. Yang, S. Zhang, D. Huang, X. Wu, H. Zhu, <strong>T. He*</strong>, S. Tang, H. Zhao, Q. Qiu, B. Lin, X. He and W. Ouyang. <br />
CVPR 2024,
<a href='https://arxiv.org/pdf/2310.08370.pdf'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="taseg"><img class="paper" src="papers/taseg.png" title="TASeg: Temporal Aggregation Network for LiDAR Semantic Segmentation
" />
<div> <strong>TASeg: Temporal Aggregation Network for LiDAR Semantic Segmentation
</strong><br />
X Wu, Y Hou, X Huang, B Lin, <strong>T. He</strong>, X Zhu, Y Ma, B Wu, H Liu, D Cai, W Ouyang <br />
CVPR 2024,
<a href='https://arxiv.org/abs/2407.09751'>[PDF]</a>
<a href='https://github.com/LittlePey/TASeg'>[code]</a>
</div>
<div class="spanner"></div>
</div>





<div class="paper" id="dreamcomposer"><img class="paper" src="papers/dreamcomposer.png" title="DreamComposer: Controllable 3D Object Generation via Multi-View Conditions" />
<div> <strong>DreamComposer: Controllable 3D Object Generation via Multi-View Conditions
</strong><br />
Y. Yang, Y. Huang, X. Wu, Y. Guo, S. Zhang, H. Zhao, <strong>T. He</strong> and X. Liu. <br />
CVPR 2024,
<a href='https://arxiv.org/pdf/2312.03611.pdf'>[PDF]</a>
<a href='https://yhyang-myron.github.io/DreamComposer/'>[code]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ptv3"><img class="paper" src="papers/ptv3.png" title="Point Transformer V3: Simpler, Faster, Stronger" />
<div> <strong>Point Transformer V3: Simpler, Faster, Stronger
</strong><br />
X. Wu, L. Jiang, P. Wang, Z. Liu, X. Liu, Y. Qiao, W. Ouyang, <strong>T. He</strong> and H. Zhao. <br />
CVPR 2024,
<a href='https://arxiv.org/pdf/2312.10035.pdf'>[PDF]</a>
<a href='https://github.com/Pointcept/PointTransformerV3'>[code]</a>
</div>
<div class="spanner"></div>
</div>





<div class="paper" id="partialft"><img class="paper" src="papers/partialft.png" title="Partial Fine-Tuning: A Successor to Full Fine-Tuning for Vision Transformers" />
<div> <strong>Partial Fine-Tuning: A Successor to Full Fine-Tuning for Vision Transformers
</strong><br />
P. Ye, Y. Huang, C. Tu, M. Li, T. Chen, <strong>T. He</strong> and W. Ouyang. <br />
arxiv 2023,
<a href='https://arxiv.org/pdf/2312.15681.pdf'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="frozenclip3d"><img class="paper" src="papers/frozen_clip3d.png" title="Frozen CLIP Model is An Efficient Point Cloud Backbone" />
<div> <strong>Frozen CLIP Model is An Efficient Point Cloud Backbone
</strong><br />
X. Huang, S. Li, W. Qu, <strong>T. He*</strong>, Y. Zuo and W. Ouyang. <br />
AAAI 2024,
<a href='https://arxiv.org/pdf/2212.04098.pdf'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>





<div class="paper" id="hulk"><img class="paper" src="papers/hulk.png" title="Hulk: A Universal Knowledge Translator
for Human-Centric Tasks" />
<div> <strong>Hulk: A Universal Knowledge Translator for Human-Centric Tasks
</strong><br />
Y. Wang, Y. Wu, S. Tang, W. He, X. Guo, F. Zhu, L. Bai, R. Zhao, J. Wu, <strong>T. He</strong> and W. Ouyang. <br />
arxiv 2023,
<a href='https://arxiv.org/pdf/2312.01697.pdf'>[PDF]</a>
<a href='https://github.com/OpenGVLab/HumanBench'>[code]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="gupnet++"><img class="paper" src="papers/gupnet++.png" title="GUPNet++: Geometry Uncertainty Propagation
Network for Monocular 3D Object Detection" />
<div> <strong>GUPNet++: Geometry Uncertainty Propagation Network for Monocular 3D Object Detection
</strong><br />
Y. Lu, X. Ma, L. Yang, T. Zhang, Y. Liu, Q. Chu, <strong>T. He*</strong>, Y. Li and W. Ouyang. <br />
arxiv 2023,
<a href='https://arxiv.org/pdf/2310.15624.pdf'>[PDF]</a>
<a href='https://github.com/SuperMHP/GUPNet Plus'>[code]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="ponderv2"><img class="paper" src="papers/ponderv2.png" title="PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm" />
<div> <strong>PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm
</strong><br />
H. Zhu, H. Yang, X. Wu, D. Huang, S. Zhang, X. He, <strong>T. He*</strong>, H. Zhao, C. Shen, Y. Qiao and W. Ouyang. <br />
arxiv 2023,
<a href='https://arxiv.org/pdf/2310.08586.pdf'>[PDF]</a>
<a href='https://github.com/OpenGVLab/PonderV2'>[code]</a>
</div>
<div class="spanner"></div>
</div>





<div class="paper" id="boosting"><img class="paper" src="papers/boosting.png" title="Boosting Residual Networks with Group Knowledge" />
<div> <strong>Boosting Residual Networks with Group Knowledge
</strong><br />
S. Tang, P. Ye, B. Li, W. Lin, T. Chen, <strong>T. He</strong>, C. Yu and W. Ouyang. <br />
arxiv 2023,
<a href='https://arxiv.org/pdf/2308.13772.pdf'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="EWA"><img class="paper" src="papers/EWA.png" title="Experts Weights Averaging: A New General Training
Scheme for Vision Transformers" />
<div> <strong>Experts Weights Averaging: A New General Training Scheme for Vision Transformers
</strong><br />
Y. Huang, P. Ye, X. Huang, S. Li, T. Chen, <strong>T. He</strong> and W. Ouyang. <br />
arxiv 2023,
<a href='https://arxiv.org/pdf/2308.06093.pdf'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="longtail"><img class="paper" src="papers/arxiv_longtail.png" title="The Equalization Losses: Gradient-Driven Training for Long-tailed Object Recognition
" />
<div> <strong>The Equalization Losses: Gradient-Driven Training for Long-tailed Object Recognition
</strong><br />
J. Tan, B. Li, X. Lu, Y. Yao, F. Yu, <strong>T. He</strong>, W. Ouyang.  <br />
T-PAMI 2023
<a href='https://arxiv.org/abs/2210.05566'>[PDF]</a>
<a href='https://github.com/ModelTC/United-Perception'>[code]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="ponder"><img class="paper" src="papers/ponder.png" title="Ponder: Point Cloud Pre-training via Neural Rendering" />
<div> <strong>Ponder: Point Cloud Pre-training via Neural Rendering
</strong><br />
D. Huang, S. Peng, <strong>T. He*</strong>, X. Zhou and W. Ouyang. <br />
ICCV 2023,
<a href='https://arxiv.org/pdf/2301.00157.pdf'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="sam3d"><img class="paper" src="papers/sam3d.png" title="SAM3D: Segment Anything in 3D Scenes" />
<div> <strong>SAM3D: Segment Anything in 3D Scenes
</strong><br />
Y. Yang, X. Wu, <strong>T. He</strong>, H. Zhao and X. Liu. <br />
<a href='https://arxiv.org/pdf/2306.03908.pdf'>[PDF]</a>
<a href='https://github.com/Pointcept/SegmentAnything3D'>[code]</a>
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="pvtssd"><img class="paper" src="papers/pvtssd.png" title="PVT-SSD:Single-Stage3DObjectDetectorwithPoint-VoxelTransformer" />
<div> <strong>PVT-SSD: Single-Stage 3D Object Detector with Point-Voxel Transformer
</strong><br />
H. Yang, W. Wang, M. Chen, B. Lin*, <strong>T. He*</strong>, H. Chen, X. He and W. Ouyang. <br />
CVPR 2023,
<a href='https://arxiv.org/pdf/2305.06621.pdf'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="ST++"><img class="paper" src="papers/st++.png" title="Stimulative Training++: Go Beyond The Performance Limits of Residual Networks" />
<div> <strong>Stimulative Training++: Go Beyond The Performance Limits of Residual Networks
</strong><br />
P. Ye, <strong>T. He</strong>, S. Tang, B. Li, T. Chen, L. Bai and W. Ouyang. <br />
<a href='https://arxiv.org/pdf/2305.02507.pdf'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="cp3_arxiv"><img class="paper" src="papers/cp3_arxiv22.png" title="CP3: Unifying Point Cloud Completion by Pretrain-Prompt-Predict Paradigm" />
<div> <strong>CP3: Unifying Point Cloud Completion by Pretrain-Prompt-Predict Paradigm
</strong><br />
M. Xu, Y. Wang, Y. Liu, <strong>T. He</strong>, and Y. Qiao. <br />
T-PAMI 2023,
<a href='https://arxiv.org/pdf/2207.05359.pdf'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="GDMAE"><img class="paper" src="papers/GD-MAE.png" title="GD-MAE: Generative Decoder for MAE Pre-training on LiDAR Point Clouds" />
<div> <strong>GD-MAE: Generative Decoder for MAE Pre-training on LiDAR Point Clouds
</strong><br />
H. Yang, <strong>T. He</strong>, J. Liu, H. Chen, B. Wu, B. Lin, X. He and W. Ouyang. <br />
CVPR 2023,
<a href='https://arxiv.org/abs/2212.03010'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="mm3d"><img class="paper" src="papers/mm3d.png" title="MM-3DScene: 3D Scene Understanding by Customizing Masked Modeling with
Informative-Preserved Reconstruction and Self-Distilled Consistency" />
<div> <strong>MM-3DScene: 3D Scene Understanding by Customizing Masked Modeling with
Informative-Preserved Reconstruction and Self-Distilled Consistency
</strong><br />
M. Xu, M. Xu, <strong>T. He</strong>, W. Ouyang, Y. Wang, X. Han, and Y. Qiao. <br />
CVPR 2023,
<a href='https://arxiv.org/pdf/2212.09948.pdf'>[PDF]</a>
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="dg_ryc"><img class="paper" src="papers/dg_ryc.png" title="Crossing the Gap: Domain Generalization for Image Captioning" />
<div> <strong>Crossing the Gap: Domain Generalization for Image Captioning
</strong><br />
Y. Ren, Z. Mao, S. Fang, Y. Lu, <strong>T. He</strong>, H. DU, Y. Zhang, and W. Ouyang. <br />
CVPR 2023,
<a href='https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Crossing_the_Gap_Domain_Generalization_for_Image_Captioning_CVPR_2023_paper.pdf'>[PDF]</a>
</div>
<div class="spanner"></div>
</div>





<div class="paper" id="beta_darts"><img class="paper" src="papers/beta_darts.png" title="β-DARTS++: Bi-level Regularization for Proxy-robust Differentiable Architecture Search" />
<div> <strong>β-DARTS++: Bi-level Regularization for Proxy-robust Differentiable Architecture Search
</strong><br />
P. Ye, <strong>T. He</strong>, B. Li, T. Chen, L. Bai and W. Ouyang. <br />
arxiv 2023,
<a href='https://arxiv.org/pdf/2301.06393.pdf'>[PDF]</a>
<a href='https://github.com/Sunshine-Ye/Beta-DARTS'>[code]</a>
</div>
<div class="spanner"></div>
</div>




<div class="paper" id="obmo"><img class="paper" src="papers/OBMO.png" title="OBMO: One Bounding Box Multiple Objects
for Monocular 3D Object Detection" />
<div> <strong>OBMO: One Bounding Box Multiple Objects
for Monocular 3D Object Detection
</strong><br />
C. Huang, <strong>T. He</strong>, H. Ren, W. Wang, B. Lin, and D. Cai. <br />
arxiv 2022,
<a href='https://arxiv.org/pdf/2212.10049.pdf'>[PDF]</a>
</div>
<div class="spanner"></div>
</div>











<div class="paper" id="queryIS"><img class="paper" src="papers/3d-queryIS.png" title="3D-QueryIS: A Query-based Framework for 3D Instance Segmentation" />
<div> <strong>3D-QueryIS: A Query-based Framework for 3D Instance Segmentation
</strong><br />
J. Liu, <strong>T. He</strong>, H. Yang, R. Su, J. Tian, J. Wu, H. Guo, K. Xu and W. Ouyang. <br />
arxiv 2022,
<a href='https://arxiv.org/pdf/2211.09375.pdf'>[PDF]</a>
<a href=''>[code]</a>
</div>
<div class="spanner"></div>
</div>





<div class="paper" id="siggraphasia"><img class="paper" src="papers/siggraph_asia_2022.png" title="Reconstructing Hand-Held Objects from Monocular Video" />
<div> <strong>Reconstructing Hand-Held Objects from Monocular Video
</strong><br />
D. Huang, X. Ji, X. He, J. Sun, <strong>T. He</strong>, Q. Shuai, W. Ouyang, and X. Zhou. <br />
SIGGRAPH Asia 2022,
<a href='https://arxiv.org/abs/2211.16835'>[PDF]</a>
<a href='https://dihuangdh.github.io/hhor/'>[Project Page]</a>
<a href='https://dihuangdh.github.io/hhor/'>[code]</a>
</div>
<div class="spanner"></div>
</div>




<div class="paper" id="arxiv21"><img class="paper" src="papers/arxiv21.png" title="Dynamic Convolution for 3D Point Cloud
Instance Segmentation" />
<div> <strong>Dynamic Convolution for 3D Point Cloud Instance Segmentation</strong><br />
<strong>T. He</strong>, C. Shen and A. Hengel<br /> T-PAMI, 2022
<a href='https://arxiv.org/pdf/2107.08392.pdf'>[PDF]</a>
<a href='https://github.com/aim-uofa/DyCo3D'>[code]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="arxiv22_pointinst3d"><img class="paper" src="papers/arxiv22_pointinst3d.png" title="PointInst3D: Segmenting 3D Instances by Points" />
<div> <strong>PointInst3D: Segmenting 3D Instances by Points</strong><br />
<strong>T. He</strong>, W. Yin, C. Shen, A. Hengel <br />
ECCV, 2022
<a href='https://arxiv.org/abs/2204.11402'>[PDF]</a>
<a href='https://github.com/tonghe90/PointInst3D'>[code]</a>
</div>
<div class="spanner"></div>
</div>




<div class="paper" id="tpami21"><img class="paper" src="papers/tpami21.png" title="ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text Spotting" />
<div> <strong>ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text Spotting</strong><br />
Y. Liu, C. Shen, L. Jin, <strong>T. He</strong>, P. Chen, C. Liu and H. Chen  <br />
T-PAMI, 2021
<a href='https://arxiv.org/pdf/2105.03620.pdf'>[PDF]</a>
<a href='https://github.com/aim-uofa/AdelaiDet'>[code]</a>
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="ijcv19"><img class="paper" src="papers/ijcv19.png" title="Exploring the Capacity of Sequential-free Box Discretization Network for Omnidirectional Scene Text Detection" />
<div> <strong>Exploring the Capacity of Sequential-free Box Discretization Network
for Omnidirectional Scene Text Detection</strong><br />
Y. Liu, <strong>T. He</strong>, H. Chen, X. Wang, C. Luo, S. Zhang, C. Shen and L. Jin  <br />
IJCV, 2021
<a href='https://arxiv.org/pdf/1912.09629.pdf'>[PDF]</a>
<a href='https://github.com/Yuliang-Liu/Box_Discretization_Network'>[code]</a>
</div>
<div class="spanner"></div>
</div>





<div class="paper" id="HCRF"><img class="paper" src="papers/HCRF.png" title="HCRF-Flow: Scene Flow from Point Clouds with Continuous High-order CRFs and Position-aware Flow Embedding" />
<div> <strong>HCRF-Flow: Scene Flow from Point Clouds with Continuous High-order CRFs and Position-aware Flow Embedding</strong><br />
R. Li, G. Lin, <strong>T. He</strong>, F. Liu and C. Shen <br />
CVPR, 2021
<a href='https://arxiv.org/abs/2105.07751'>[PDF]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="dyco3d"><img class="paper" src="papers/dyco3d.png" title="DyCo3D: Robust Instance Segmentation of 3D Point Clouds through Dynamic Convolution" />
<div> <strong>DyCo3D: Robust Instance Segmentation of 3D Point Clouds through Dynamic Convolution</strong><br />
<strong>T. He</strong>, C. Shen, and A. Hengel <br />
CVPR, 2021
<a href='https://arxiv.org/pdf/2011.13328.pdf'>[PDF]</a>
<a href='https://github.com/aim-uofa/DyCo3D'>[Code]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="pc_mem"><img class="paper" src="papers/point_cloud_mem.png" title="Learning and Memorizing Representative Prototypes for 3D Point Cloud Semantic and Instance Segmentation" />
<div> <strong>Learning and Memorizing Representative Prototypes for 3D Point Cloud
Semantic and Instance Segmentation</strong><br />
<strong>T. He</strong>, D. Gong, Z. Tian and C. Shen <br />
ECCV, 2020
<a href='https://arxiv.org/pdf/2001.01349.pdf'>[PDF]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="pc_embedding"><img class="paper" src="papers/eccv2020_instance-aware.png" title="Learning and Memorizing Representative Prototypes for 3D Point Cloud Semantic and Instance Segmentation" />
<div> <strong>Instance-Aware Embedding for Point Cloud Instance Segmentation</strong><br />
<strong>T. He</strong>, Y. Liu, C. Shen, X. Wang and C.Sun<br />
ECCV, 2020
<a href='https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750256.pdf'>[PDF]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="fcos_tpami"><img class="paper" src="papers/fcos_tpami.png" title="FCOS: A Simple and Strong Anchor-free Object Detector" />
<div> <strong>FCOS: A Simple and Strong Anchor-free Object Detector</strong><br />
Z. Tian, C. Shen, H. Chen, <strong>T. He</strong>  <br />
T-PAMI, 2020.
<a href='https://arxiv.org/pdf/2006.09214.pdf'>[PDF]</a>
<a href='https://github.com/aim-uofa/AdelaiDet'>[Code]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="abcnet"><img class="paper" src="papers/cvpr2020_abc.png" title="ABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network" />
<div> <strong>ABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network</strong><br />
Y. Liu, H. Chen, C. Shen, <strong>T. He</strong>, L. Jin, L. Wang<br />
CVPR 2020
<a href='https://arxiv.org/pdf/2002.10200.pdf'>[PDF]</a>
<a href='https://github.com/aim-uofa/AdelaiDet'>[Code]</a>
</div>
<div class="spanner"></div>
</div>







<div class="paper" id="arxiv19"><img class="paper" src="papers/fcos.png" title="FCOS: Fully Convolutional One-Stage Object Detection" />
<div> <strong>FCOS: Fully Convolutional One-Stage Object Detection</strong><br />
Z. Tian, C. Shen, H. Chen, <strong>T. He</strong>  <br />
ICCV, 2019
<a href='https://arxiv.org/abs/1904.01355'>[PDF]</a>
<a href='https://github.com/tianzhi0549/FCOS'>[Code]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="CVPR2019"><img class="paper" src="papers/cvpr_2019.png" title="Knowledge Translation and Adaptation for Efficient Semantic Segmentation" />
<div> <strong>Knowledge Translation and Adaptation for Efficient Semantic Segmentation</strong><br />
<strong>T. He</strong>, C. Shen, Z. Tian, D. Gong, C. Sun, Y. Yan  <br />
CVPR, 2019
<a href='https://arxiv.org/abs/1903.04688'>[PDF]</a>
<a href='https://www.cityscapes-dataset.com/anonymous-results/?id=4c673e69263b3eb449657b13a86261607c964db76e2151ed36fa67e4b8d2da1a'>[Results On Cityscapes Test Set]</a>

</div>
<div class="spanner"></div>
</div>

<div class="paper" id="CVPR2019"><img class="paper" src="papers/cvpr2019_tz.png" title="Decoders Matter for Semantic Segmentation:
Data-Dependent Decoding Enables Flexible Feature Aggregation" />
<div> <strong>Decoders Matter for Semantic Segmentation:
Data-Dependent Decoding Enables Flexible Feature Aggregation</strong><br />
Z. Tian, <strong>T. He</strong>, C. Shen, Y. You  <br />
CVPR, 2019
<a href='https://arxiv.org/pdf/1903.02120'>[PDF]</a>


</div>
<div class="spanner"></div>
</div>

<div class="paper" id="CVPR2018"><img class="paper" src="papers/cvpr2018.png" title="An End-to-End Text Detector with Regional Attention" />
<div> <strong>An End-to-End TextSpotter with Explicit Alignment and Attention</strong><br />
<strong>T. He</strong>, Z. Tian, W. Huang, C. Shen, Y. Qiao, C. Sun  <br />
CVPR, 2018
<a href='https://arxiv.org/abs/1803.03474'>[PDF]</a>
<a href='https://github.com/tonghe90/textspotter'>[code]</a>

</div>
<div class="spanner"></div>
</div>


<div class="paper" id="ICCV2017"><img class="paper" src="papers/ICCV2017.png" title="Single Shot Text Detector with Regional Attention" />
<div> <strong>Single Shot Text Detector with Regional Attention</strong><br />
P. He, W. Huang, <strong>T. He</strong>, Q. Zhu, Y. Qiao, X. Li  <br />
ICCV, 2017
<a href='https://arxiv.org/pdf/1709.00138.pdf'>[PDF]</a>
<a href='https://github.com/BestSonny/SSTD'>[code]</a>

</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICCV2017"><img class="paper" src="papers/ccbr_2017.png" title="Orientation-Aware Text Proposals Network for Scene Text Detection" />
<div> <strong>Orientation-Aware Text Proposals Network for Scene Text Detection</strong><br />
H. Huang, Z. Tian, <strong>T. He</strong>, W. Huang, Y. Qiao <br />
CCBR, 2017

</div>
<div class="spanner"></div>
</div>



<div class="paper" id="CVPR16"><img class="paper" src="papers/ECCV16.png" title="Detecting Text in Natural Image with Connectionist Text Proposal Network" />
<div> <strong>Detecting Text in Natural Image with Connectionist Text Proposal Network</strong><br />
T. Zhi, W. Huang, <strong>T. He</strong>, P. He, Y. Qiao   <br />
ECCV, 2016
<a href='http://textdet.com/'>[demo]</a>
<a href='https://github.com/tianzhi0549/CTPN'>[code]</a> <br />

</div>
<div class="spanner"></div>
</div>

<div class="paper" id="TIP"><img class="paper" src="papers/arxiv.png" title="Accurate Text Localization in Natural Image with Cascaded Convolutional Text Network" />
<div> <strong>Accurate Text Localization in Natural Image with Cascaded Convolutional Text Network</strong><br />
<strong>T. He</strong>, W. Huang, Y. Qiao and J.Yao   <br />
arxiv <a href='http://arxiv.org/abs/1603.09423v1'>[arxiv 1510.03283]</a><br /> 
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="TIP"><img class="paper" src="papers/textcnn_arxiv2015.png" title="Text-Attentional Convolutional Neural Networks for Scene Text Detection" />
<div> <strong>Text-Attentional Convolutional Neural Networks for Scene Text Detection</strong><br />
<strong>T. He</strong>, W. Huang, Y. Qiao and J.Yao   <br />
T-IP 2016 <a href='http://arxiv.org/abs/1510.03283'>[arxiv 1510.03283]</a><br /> 

</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICIA2015"><img class="paper" src="papers/ICIA.png" title="An efficient method for text detection from indoor panorama images using extremal regions" />
<div> <strong>An efficient method for text detection from indoor panorama images using extremal regions</strong><br />
Y. Liu, K. Zhang, J. Yao, <strong>T. He</strong>, Y. Liu and J. Tu <br />
ICIA, 2015.
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ITSC2014"><img class="paper" src="papers/ITSC2014.jpg" title="Accurate Multi-Scale License Plate Localization Via Image Saliency" />
<div> <strong>Accurate Multi-Scale License Plate Localization Via Image Saliency</strong><br />
<strong>T. He</strong>, J. Yao, K. Zhang, Y. Hou and S. Han <br />
ITSC, 2014. <alert>[oral]</alert>
</div>
<div class="spanner"></div>
</div>

</div>
</div>


<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Professional activities</h2>
<div class="paper">
<ul>
<p><strong><font size="5"> Journals</font></p></strong>
<p><font size="5">Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</font></p>
<p><font size="5">International Journal of Computer Vision (IJCV)</font></p>
<p><font size="5">Transaction on Image Processing(TIP)</font></p>
<p><font size="5">Pattern Recognition(PR)</font></p>
<p><font size="5"> IEEE Transactions on Circuits and Systems for Video Technology(TCSVT)</font></p>
<p><strong><font size="5"> Conferences</font></p></strong>
<p><font size="5">CVPR, ICCV, ECCV, NIPS, ICLR, AAAI, etc.</font></p>
</ul>
</div>
</div>
</div>

<!--
<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Contests</h2>
<div class="paper">
<ul>
<li><strong>Large-scale Scene Understanding Challenge: <a href='http://gesture.chalearn.org/'>Scene Classification</a></strong> (<small>CVPR2015 workshop</small>),  <strong>Rank</strong>: 2/4</li>
<li><strong>National Graduate Contest on Smart-City Technology and Creative Design: <a href='http://avid.erangelab.com/'>Face Detection</a></strong>,  <strong>Rank</strong>: 2/10.</li>

<li><strong>The First-class Prize of China Undergraduate Mathematical Contest in Modeling</strong> </li>

</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section"><h2>Awards</h2>
<div class="paper">
<li><strong>“National Scholarship”, 2013/2014</li>
</div>
</div>
</div>
-->





<!--
<div style="clear: both;">
<div class="section"><h2>Friends</h2>
<div class="paper">
<a href='https://github.com/tianzhi0549'>Zhi Tian</a> (Adelaide Uni),
<a href='http://bestsonny.github.io'>Pan He</a> (UF),
<a href="http://wangzheallen.github.io/">Zhe Wang</a> (UCI),
<a href="http://ydwen.github.io/">Yandong Wen</a> (CMU),   
<a href="http://zbwglory.github.io/">Bowen Zhang</a> (USC), 
<a href="http://kpzhang93.github.io/">Kaipeng Zhang</a> (UToyko),
<a href="http://guoshengcv.github.io/">Sheng Guo</a> (MALONG),
<a href="http://wanglimin.github.io/">Limin Wang</a> (ETHz),
</div>
</div>
</div>
-->

<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 26th Aug, 2019</a></font></p>
<p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>

<hr>
<div id="clustrmaps-widget"></div><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=2qC7&d=moSsaMzezP5mVgywsQ3_t_xX8z9yNx7f1iPUVR5tuQE"></script>

</body>
</html>
